{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b2371f63-92b9-4dba-ba43-f530b10ebbc5",
   "metadata": {},
   "source": [
    "# Libraries & Function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5581c8c3-15b0-40bc-b4b4-251d49c11f1d",
   "metadata": {},
   "source": [
    "## Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "78845726-e3d2-43a0-b7f9-4f80b4a0497d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\npip install tifffile, \\npip install geojson, \\npip install scikit-image, \\npip install -U albumentations, \\npip install opencv-python==4.8.0.74 \\npip install ipywidgets --upgrade\\npip install notebook --upgrade \\npip install opencv-contrib-python\\npip install imagecodecs\\n\\n'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "pip install tifffile, \n",
    "pip install geojson, \n",
    "pip install scikit-image, \n",
    "pip install -U albumentations, \n",
    "pip install opencv-python==4.8.0.74 \n",
    "pip install ipywidgets --upgrade\n",
    "pip install notebook --upgrade \n",
    "pip install opencv-contrib-python\n",
    "pip install imagecodecs\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "35fbae6a-bcbb-4e52-a776-e70a4c377450",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Requirement already satisfied: tifffile in /usr/local/lib/python3.10/dist-packages (2024.9.20)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from tifffile) (1.24.4)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable.It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install tifffile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "af685848-0c9a-4ae2-bc4b-380e7f529e13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Requirement already satisfied: geojson in /usr/local/lib/python3.10/dist-packages (3.1.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable.It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install geojson"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e66c2c36-6d86-4c29-844e-327bc9295c00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Requirement already satisfied: albumentations in /usr/local/lib/python3.10/dist-packages (1.4.18)\n",
      "Requirement already satisfied: numpy>=1.24.4 in /usr/local/lib/python3.10/dist-packages (from albumentations) (1.24.4)\n",
      "Requirement already satisfied: scipy>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from albumentations) (1.14.0)\n",
      "Requirement already satisfied: scikit-image>=0.21.0 in /usr/local/lib/python3.10/dist-packages (from albumentations) (0.24.0)\n",
      "Requirement already satisfied: PyYAML in /usr/local/lib/python3.10/dist-packages (from albumentations) (6.0.2)\n",
      "Requirement already satisfied: pydantic>=2.7.0 in /usr/local/lib/python3.10/dist-packages (from albumentations) (2.8.2)\n",
      "Requirement already satisfied: albucore==0.0.17 in /usr/local/lib/python3.10/dist-packages (from albumentations) (0.0.17)\n",
      "Requirement already satisfied: eval-type-backport in /usr/local/lib/python3.10/dist-packages (from albumentations) (0.2.0)\n",
      "Requirement already satisfied: opencv-python-headless>=4.9.0.80 in /usr/local/lib/python3.10/dist-packages (from albumentations) (4.10.0.84)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic>=2.7.0->albumentations) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.20.1 in /usr/local/lib/python3.10/dist-packages (from pydantic>=2.7.0->albumentations) (2.20.1)\n",
      "Requirement already satisfied: typing-extensions>=4.6.1 in /usr/local/lib/python3.10/dist-packages (from pydantic>=2.7.0->albumentations) (4.12.2)\n",
      "Requirement already satisfied: networkx>=2.8 in /usr/local/lib/python3.10/dist-packages (from scikit-image>=0.21.0->albumentations) (3.3)\n",
      "Requirement already satisfied: pillow>=9.1 in /usr/local/lib/python3.10/dist-packages (from scikit-image>=0.21.0->albumentations) (10.4.0)\n",
      "Requirement already satisfied: imageio>=2.33 in /usr/local/lib/python3.10/dist-packages (from scikit-image>=0.21.0->albumentations) (2.36.0)\n",
      "Requirement already satisfied: tifffile>=2022.8.12 in /usr/local/lib/python3.10/dist-packages (from scikit-image>=0.21.0->albumentations) (2024.9.20)\n",
      "Requirement already satisfied: packaging>=21 in /usr/local/lib/python3.10/dist-packages (from scikit-image>=0.21.0->albumentations) (23.2)\n",
      "Requirement already satisfied: lazy-loader>=0.4 in /usr/local/lib/python3.10/dist-packages (from scikit-image>=0.21.0->albumentations) (0.4)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable.It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install albumentations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "26c5c495-18e8-47b9-9bdb-4f240ac40f26",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\npip install tifffile, \\npip install geojson, \\npip install scikit-image, \\npip install -U albumentations, \\npip install opencv-python==4.8.0.74 \\npip install ipywidgets --upgrade\\npip install notebook --upgrade \\npip install opencv-contrib-python\\npip install imagecodecs\\n\\n'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "pip install tifffile, \n",
    "pip install geojson, \n",
    "pip install scikit-image, \n",
    "pip install -U albumentations, \n",
    "pip install opencv-python==4.8.0.74 \n",
    "pip install ipywidgets --upgrade\n",
    "pip install notebook --upgrade \n",
    "pip install opencv-contrib-python\n",
    "pip install imagecodecs\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f2369f9a-fadb-46bb-bf73-190e8072b349",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('utility_box/')\n",
    "from torch_gpu_utils import get_device\n",
    "from image_utils import plot_image_series, plot_overlay, plot_image, plot_image_series, plot_overlay_series\n",
    "\n",
    "import random\n",
    "import pandas as pd\n",
    "from tqdm.auto import tqdm\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "\n",
    "import albumentations as A\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from dataloader_with_augs import CamelyonDataset\n",
    "\n",
    "from network_definition import UNet\n",
    "from loss_functions import get_dice_loss\n",
    "\n",
    "import segmentation_models_pytorch as smp\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a46a331-2fa9-4487-94d0-62e267270972",
   "metadata": {},
   "source": [
    "# Utility Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "70d1919d-5562-48b3-afaa-006e19b61dc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model_logdir(model_logs_dir):\n",
    "    \"\"\"\n",
    "    Creates a directory structure for model logging and checkpoints in the specified main folder.\n",
    "    \n",
    "    The structure created is as follows:\n",
    "    ./<main_folder_name>/\n",
    "        train_logs/\n",
    "            train_images/\n",
    "        val_logs/\n",
    "            val_images/\n",
    "        model_check_points/\n",
    "    \n",
    "    Args:\n",
    "    main_folder_name (str): The name of the main folder where the directory structure will be created.\n",
    "    \"\"\"\n",
    "    # Get the current working directory\n",
    "    base_path = Path.cwd()\n",
    "\n",
    "    # Define the directory structure using Path objects and the main folder name\n",
    "    dir_structure = [\n",
    "        base_path / model_logs_dir / 'train_logs' / 'train_images',\n",
    "        base_path / model_logs_dir / 'val_logs' / 'val_images',\n",
    "        base_path / model_logs_dir / 'model_check_points' / 'max_val',\n",
    "        base_path / model_logs_dir / 'model_check_points' / 'recurrent'\n",
    "    ]\n",
    "    \n",
    "    # Create the directories\n",
    "    for dir_path in dir_structure:\n",
    "        dir_path.mkdir(parents=True, exist_ok=True)\n",
    "        print(f\"Created directory: {dir_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2afe1136-82e6-4223-aa48-74866bd39d70",
   "metadata": {},
   "outputs": [],
   "source": [
    "def crop_masks(masks):\n",
    "    return masks\n",
    "\n",
    "def crop_images(images):\n",
    "    return images"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a23170bb-66a9-4bbb-bafb-d040d70bda0b",
   "metadata": {},
   "source": [
    "## Train Logic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5ba0f0c0-52c7-449b-ac5f-3edf875e4b0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_logic(model, epoch, train_loader, model_logs_dir):\n",
    "    model.train()\n",
    "    batch_train_logs=[]\n",
    "    train_pbar=tqdm(total=len(train_loader), desc='training')\n",
    "    train_epoch_loss=0\n",
    "    save_image_counter=0\n",
    "    for batch_idx, batch in enumerate(train_loader):\n",
    "        images=batch[0].to(device) \n",
    "        true_masks=batch[1].to(device)\n",
    "        true_masks=crop_masks(true_masks)\n",
    "        \n",
    "        with torch.autocast(device.type if device.type != 'mps' else 'cpu', enabled=amp):\n",
    "            pred_masks=model(images)\n",
    "            \n",
    "        if n_classes == 1:\n",
    "            loss=criterion(pred_masks.squeeze(1), true_masks.float())\n",
    "\n",
    "            prob_masks = F.sigmoid(pred_masks)\n",
    "            thresh_masks = (prob_masks > 0.5).float()\n",
    "            \n",
    "            dice_loss=get_dice_loss(thresh_masks.squeeze(1), true_masks.float(), multiclass=False)\n",
    "            loss+=dice_loss\n",
    "            \n",
    "        else:\n",
    "            loss = criterion(pred_masks, true_masks)\n",
    "            dice_loss=get_dice_loss(\n",
    "                F.softmax(pred_masks, dim=1).float(),\n",
    "                F.one_hot(true_masks, model.n_classes).permute(0, 3, 1, 2).float(),\n",
    "                multiclass=True\n",
    "            )\n",
    "            loss+=dice_loss\n",
    "        \n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        grad_scaler.scale(loss).backward()\n",
    "        grad_scaler.unscale_(optimizer)\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), gradient_clipping)\n",
    "        grad_scaler.step(optimizer)\n",
    "        grad_scaler.update()\n",
    "\n",
    "        train_epoch_loss += loss.item()\n",
    "\n",
    "        train_temp_dict={}\n",
    "        train_temp_dict['epoch']=epoch\n",
    "        train_temp_dict['batch_idx']=batch_idx\n",
    "        train_temp_dict['loss']=loss.item()\n",
    "        train_temp_dict['dice_loss']=dice_loss.item()\n",
    "        train_temp_dict['n_samples']=images.shape[0]\n",
    "        batch_train_logs.append(train_temp_dict)\n",
    "        \n",
    "        if batch_idx%2==0:\n",
    "            #randomly save a prediction and its true values for progress tracking\n",
    "            random_idx=random.randint(0, len(images)-1)\n",
    "            true_mask=crop_masks((true_masks[random_idx].to('cpu').detach().numpy()).astype(np.uint8))\n",
    "            thresh_mask=thresh_masks[random_idx].squeeze(0).to('cpu').detach().numpy().astype(np.uint8)\n",
    "            image=crop_images(images[random_idx].to('cpu').numpy().transpose(1,2,0))\n",
    "\n",
    "            image_series_path=Path(f\"{model_logs_dir}/train_logs/train_images/epoch{epoch}/epoch{epoch}_batch{batch_idx}_image{save_image_counter}.jpg\")\n",
    "            overlay_series_path=Path(f\"{model_logs_dir}/train_logs/train_images/epoch{epoch}/overlay_epoch{epoch}_batch{batch_idx}_image{save_image_counter}.jpg\")\n",
    "            image_series_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "            overlay_series_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "            \n",
    "            plot_image_series([true_mask,thresh_mask,image], \n",
    "                              ['true_mask', 'thresh_mask', 'image'],\n",
    "                              save_path=image_series_path,\n",
    "                              plot=False\n",
    "                             )\n",
    "            plot_overlay_series([image, image, image],\n",
    "                                [true_mask, thresh_mask, torch.zeros(image.shape[:2])],\n",
    "                                ['true_mask', 'thresh_mask', 'image'],\n",
    "                                save_path=overlay_series_path,\n",
    "                                plot=False)\n",
    "            \n",
    "            save_image_counter+=1\n",
    "\n",
    "        pd.DataFrame(batch_train_logs).to_csv(f\"{model_logs_dir}/train_logs/epoch{epoch}_batch_train_logs.csv\", index=False)\n",
    "        train_pbar.update()\n",
    "    \n",
    "    return model, train_epoch_loss, pd.DataFrame(batch_train_logs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "701e454a-6c99-4f6d-9ffb-8397922d2df3",
   "metadata": {},
   "source": [
    "## Validation Logic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e50898d4-714d-4158-bcb4-df2089db829a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def val_logic(model, epoch, val_loader, model_logs_dir):\n",
    "    model.eval()\n",
    "    batch_val_logs=[]\n",
    "    val_pbar=tqdm(total=len(val_loader), desc='validating')\n",
    "    val_epoch_loss=0\n",
    "    save_image_counter=0\n",
    "    \n",
    "    for batch_idx, batch in enumerate(val_loader):\n",
    "        images=batch[0].to(device) \n",
    "        true_masks=batch[1].to(device)\n",
    "        true_masks=crop_masks(true_masks)\n",
    "\n",
    "        with torch.inference_mode():\n",
    "            with torch.autocast(device.type if device.type != 'mps' else 'cpu', enabled=amp):\n",
    "                pred_masks=model(images)\n",
    "\n",
    "            if n_classes == 1:\n",
    "                val_loss=criterion(pred_masks.squeeze(1), true_masks.float())\n",
    "\n",
    "                prob_masks = F.sigmoid(pred_masks)\n",
    "                thresh_masks = (prob_masks > 0.5).float()\n",
    "                \n",
    "                val_dice_loss=get_dice_loss(thresh_masks.squeeze(1), true_masks.float(), multiclass=False)\n",
    "                val_loss+=val_dice_loss\n",
    "            else:\n",
    "                val_loss = criterion(pred_masks, true_masks)\n",
    "                val_dice_loss=get_dice_loss(\n",
    "                    F.softmax(pred_masks, dim=1).float(),\n",
    "                    F.one_hot(true_masks, model.n_classes).permute(0, 3, 1, 2).float(),\n",
    "                    multiclass=True\n",
    "                )\n",
    "                val_loss+=val_dice_loss\n",
    "\n",
    "        val_epoch_loss += val_loss.item()\n",
    "\n",
    "        val_temp_dict={}\n",
    "        val_temp_dict['epoch']=epoch\n",
    "        val_temp_dict['batch_idx']=batch_idx\n",
    "        val_temp_dict['loss']=val_loss.item()\n",
    "        val_temp_dict['dice_loss']=val_dice_loss.item()\n",
    "        val_temp_dict['n_samples']=images.shape[0]\n",
    "        batch_val_logs.append(val_temp_dict)\n",
    "        \n",
    "        #randomly save a prediction and its true values for progress tracking\n",
    "        if batch_idx%2==0:\n",
    "            random_idx=random.randint(0, len(images)-1)\n",
    "            true_mask=crop_masks((true_masks[random_idx].to('cpu').detach().numpy()>0.5).astype(np.uint8))\n",
    "            thresh_mask=thresh_masks[random_idx].squeeze(0).to('cpu').detach().numpy().astype(np.uint8)\n",
    "            image=crop_images(images[random_idx].to('cpu').numpy().transpose(1,2,0))\n",
    "\n",
    "            image_series_path=Path(f\"{model_logs_dir}/val_logs/val_images/epoch{epoch}/epoch{epoch}_batch{batch_idx}_image{save_image_counter}.jpg\")\n",
    "            overlay_series_path=Path(f\"{model_logs_dir}/val_logs/val_images/epoch{epoch}/overlay_epoch{epoch}_batch{batch_idx}_image{save_image_counter}.jpg\")\n",
    "            \n",
    "            image_series_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "            overlay_series_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "            \n",
    "            plot_image_series([true_mask,thresh_mask,image],\n",
    "                              ['true_mask', 'thresh_mask', 'image'],\n",
    "                              save_path=image_series_path,\n",
    "                              plot=False\n",
    "                             )\n",
    "            plot_overlay_series([image,image,image],\n",
    "                                [true_mask,thresh_mask,torch.zeros(image.shape[:2])],\n",
    "                                ['true_mask', 'thresh_mask', 'image'],\n",
    "                                save_path=overlay_series_path,\n",
    "                                plot=False)\n",
    "            save_image_counter+=1\n",
    "\n",
    "        pd.DataFrame(batch_val_logs).to_csv(f\"{model_logs_dir}/val_logs/epoch{epoch}_batch_val_logs.csv\", index=False)\n",
    "        val_pbar.update()\n",
    "        \n",
    "    return model, val_epoch_loss, pd.DataFrame(batch_val_logs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dd08ea4-8ecc-49cb-a590-c9e4f2267cdd",
   "metadata": {},
   "source": [
    "## Setting Augmentations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "402a7a95-99f0-4bb7-b47a-104ddacf2663",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_training_augmentation():\n",
    "    scale = 0.25 #Maximum Should be 0.5, Downscale\n",
    "    \n",
    "    scale_setting = 0.25 #Color Jitter\n",
    "    scale_color = 0.1\n",
    "    \n",
    "    augmentations = [\n",
    "        A.RandomRotate90(p=0.5),\n",
    "        A.HorizontalFlip(p=0.5),\n",
    "        A.VerticalFlip(p=0.5),\n",
    "        A.Blur(p=0.1, blur_limit=9),\n",
    "        A.GaussNoise(p=0.2, var_limit=10),\n",
    "        A.ColorJitter(p=0.5,brightness=scale_setting,contrast=scale_setting,saturation=scale_color,hue=scale_color / 2,),\n",
    "        A.RandomBrightnessContrast(p=0.2),\n",
    "    ]\n",
    "    \n",
    "    augmentations = A.Compose(augmentations)\n",
    "    return augmentations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6e06e18a-59ab-406b-a864-0d3be4a0e910",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_augs=get_training_augmentation()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e000e40-1135-4f82-b60a-651d4b42b866",
   "metadata": {},
   "source": [
    "# Setting up the arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a6f7646d-012c-4a15-9b40-02649e7bc49d",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs=2\n",
    "total_train_patches=50\n",
    "total_val_patches=50\n",
    "\n",
    "model_logs_dir='model_logs/trial3'\n",
    "\n",
    "batch_size=10\n",
    "patch_size=512\n",
    "overlap=184\n",
    "\n",
    "target_mpp=1\n",
    "gradient_clipping=1.0\n",
    "weight_decay=1e-8\n",
    "learning_rate=1e-4\n",
    "amp=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "5d724d72-be78-42c0-8d84-292618046ba2",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_distribution=pd.read_csv('distributions/train_distribution.csv')\n",
    "val_distribution=pd.read_csv('distributions/val_distribution.csv')\n",
    "test_distribution=pd.read_csv('distributions/test_distribution.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "2cce704e-f18b-46e4-bebd-46fa046e3f24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created directory: /workspace/code/NodeSeg/model_logs/trial3/train_logs/train_images\n",
      "Created directory: /workspace/code/NodeSeg/model_logs/trial3/val_logs/val_images\n",
      "Created directory: /workspace/code/NodeSeg/model_logs/trial3/model_check_points/max_val\n",
      "Created directory: /workspace/code/NodeSeg/model_logs/trial3/model_check_points/recurrent\n"
     ]
    }
   ],
   "source": [
    "create_model_logdir(model_logs_dir)\n",
    "device=get_device(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "535c244c-fa22-4284-9daa-c7388ade9179",
   "metadata": {},
   "source": [
    "## Data Loading Modes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "39a21954-b401-4a74-ace7-fbfc3879df8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset_args_dict1 = {\n",
    "    'data_distribution': train_distribution,\n",
    "    'patch_size': patch_size,\n",
    "    'overlap': overlap,\n",
    "    'target_mpp': target_mpp,\n",
    "    'training_mode': 'pure_positive',\n",
    "    'slice_tumor': False,\n",
    "    'total_patches': total_train_patches,\n",
    "    'sample_without_replacement': False,\n",
    "    'n_sample_rows': 40\n",
    "}\n",
    "\n",
    "train_dataset_args_dict2 = {\n",
    "    'data_distribution': train_distribution,\n",
    "    'patch_size': patch_size,\n",
    "    'overlap': overlap,\n",
    "    'target_mpp': target_mpp,\n",
    "    'training_mode': 'pure_negative',\n",
    "    'slice_tumor': False,\n",
    "    'total_patches': total_train_patches,\n",
    "    'sample_without_replacement': False,\n",
    "    'n_sample_rows': 40\n",
    "}\n",
    "\n",
    "train_dataset_args_dict3 = {\n",
    "    'data_distribution': train_distribution,\n",
    "    'patch_size': patch_size,\n",
    "    'overlap': overlap,\n",
    "    'target_mpp': target_mpp,\n",
    "    'training_mode': 'mixed',\n",
    "    'slice_tumor': False,\n",
    "    'total_patches': total_train_patches,\n",
    "    'sample_without_replacement': False,\n",
    "    'n_sample_rows': 40\n",
    "}\n",
    "\n",
    "train_dataset_args_dict4 = {\n",
    "    'data_distribution': train_distribution,\n",
    "    'patch_size': patch_size,\n",
    "    'overlap': overlap,\n",
    "    'target_mpp': target_mpp,\n",
    "    'training_mode': 'pure_positive',\n",
    "    'slice_tumor': True,\n",
    "    'total_patches': total_train_patches,\n",
    "    'sample_without_replacement': False,\n",
    "    'n_sample_rows': 1\n",
    "}\n",
    "\n",
    "val_dataset_args_dict1 = {\n",
    "    'data_distribution': val_distribution,\n",
    "    'patch_size': patch_size,\n",
    "    'overlap': overlap,\n",
    "    'target_mpp': target_mpp,\n",
    "    'training_mode': 'pure_positive',\n",
    "    'slice_tumor': False,\n",
    "    'total_patches': total_val_patches,\n",
    "    'sample_without_replacement': False,\n",
    "    'n_sample_rows': 10\n",
    "}\n",
    "\n",
    "val_dataset_args_dict2 = {\n",
    "    'data_distribution': val_distribution,\n",
    "    'patch_size': patch_size,\n",
    "    'overlap': overlap,\n",
    "    'target_mpp': target_mpp,\n",
    "    'training_mode': 'pure_negative',\n",
    "    'slice_tumor': False,\n",
    "    'total_patches': total_val_patches,\n",
    "    'sample_without_replacement': False,\n",
    "    'n_sample_rows': 10\n",
    "}\n",
    "\n",
    "val_dataset_args_dict3 = {\n",
    "    'data_distribution': val_distribution,\n",
    "    'patch_size': patch_size,\n",
    "    'overlap': overlap,\n",
    "    'target_mpp': target_mpp,\n",
    "    'training_mode': 'mixed',\n",
    "    'slice_tumor': False,\n",
    "    'total_patches': total_val_patches,\n",
    "    'sample_without_replacement': False,\n",
    "    'n_sample_rows': 10\n",
    "}\n",
    "\n",
    "val_dataset_args_dict4 = {\n",
    "    'data_distribution': val_distribution,\n",
    "    'patch_size': patch_size,\n",
    "    'overlap': overlap,\n",
    "    'target_mpp': target_mpp,\n",
    "    'training_mode': 'pure_positive',\n",
    "    'slice_tumor': True,\n",
    "    'total_patches': total_val_patches,\n",
    "    'sample_without_replacement': False,\n",
    "    'n_sample_rows': 1\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3fc18974-0bb4-4bdb-8e45-1f760b6dae36",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset_args_choices=[train_dataset_args_dict1, train_dataset_args_dict2, train_dataset_args_dict3, train_dataset_args_dict4]\n",
    "val_dataset_args_choices=[val_dataset_args_dict1, val_dataset_args_dict2, val_dataset_args_dict3, val_dataset_args_dict4]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aebcc0c5-8ed8-4385-8e90-ac23d06c0bea",
   "metadata": {},
   "source": [
    "# Setting up the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8e796b76-9dc7-426b-8c14-d33d2770ef7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_classes=1\n",
    "model = smp.Unet(\n",
    "    encoder_name=\"resnet34\",        # choose encoder, e.g. mobilenet_v2 or efficientnet-b7\n",
    "    encoder_weights=\"imagenet\",     # use `imagenet` pre-trained weights for encoder initialization\n",
    "    in_channels=3,                  # model input channels (1 for gray-scale images, 3 for RGB, etc.)\n",
    "    classes=n_classes,                      # model output channels (number of classes in your dataset)\n",
    ")\n",
    "model.to(device);\n",
    "model=model.to(memory_format=torch.channels_last)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f0a7421e-d9f1-477e-bf15-ba9388005a59",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer=torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "scheduler=torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'max', patience=10)  # goal: minimize validation loss\n",
    "grad_scaler=torch.amp.GradScaler(enabled=amp)\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60b14d66-8db7-4bbf-a886-6360db442af6",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "dcaebb27-1ed2-4f32-837b-44e67050abd1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b3595d0694f743bd890d328a5d6553aa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ead674adc7984346bab10384693d9def",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sampling Rows:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "62cf23ed7d364958b760b3371faaafe7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "training:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ef609bf3c8ff4b17a468f5efff784a7e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sampling Rows:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a0ae15cc324e4d928cee98b1c0667b48",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "validating:   0%|          | 0/22 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max val score initialised to 0.44 for epoch 1.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "64b2b0dbdb1e41809dffa1bcfe0570cd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sampling Rows:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0ded697ec052418e90c14d96f580eab5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "training:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ddb8e08a1f8e4cfca142ec5077752e91",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sampling Rows:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "489ddb6e532a48a28499fac86cd33961",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "validating:   0%|          | 0/80 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max val score changed to 0.57 for epoch 2.\n"
     ]
    }
   ],
   "source": [
    "epoch_logs=[]\n",
    "for epoch in tqdm(range(1,epochs+1)):\n",
    "    try:\n",
    "        cycle_epoch = epoch % 50\n",
    "        \n",
    "        if 1 <= cycle_epoch <= 10:\n",
    "            #train_dataset_args_dict=random.choice(train_dataset_args_choices)\n",
    "            train_dataset_args_dict=train_dataset_args_choices[3]\n",
    "            train_dataset=CamelyonDataset(**train_dataset_args_dict, augmentations=train_augs)\n",
    "            train_loader=DataLoader(train_dataset, batch_size=batch_size, shuffle=False, num_workers=8)\n",
    "            model, train_epoch_loss, batch_train_logs_df=train_logic(model, epoch, train_loader, model_logs_dir)\n",
    "    \n",
    "            #val_dataset_args_dict=random.choice(val_dataset_args_choices)\n",
    "            val_dataset_args_dict=val_dataset_args_choices[3]\n",
    "            val_dataset=CamelyonDataset(**val_dataset_args_dict)\n",
    "            val_loader=DataLoader(val_dataset, batch_size = batch_size, shuffle=False, num_workers=8)\n",
    "            model, val_epoch_loss, batch_val_logs_df = val_logic(model, epoch, val_loader, model_logs_dir)\n",
    "        elif 11 <= cycle_epoch <= 20:\n",
    "            #train_dataset_args_dict=random.choice(train_dataset_args_choices)\n",
    "            train_dataset_args_dict=train_dataset_args_choices[2]\n",
    "            train_dataset=CamelyonDataset(**train_dataset_args_dict, augmentations=train_augs)\n",
    "            train_loader=DataLoader(train_dataset, batch_size=batch_size, shuffle=False, num_workers=8)\n",
    "            model, train_epoch_loss, batch_train_logs_df=train_logic(model, epoch, train_loader, model_logs_dir)\n",
    "    \n",
    "            #val_dataset_args_dict=random.choice(val_dataset_args_choices)\n",
    "            val_dataset_args_dict=val_dataset_args_choices[2]\n",
    "            val_dataset=CamelyonDataset(**val_dataset_args_dict)\n",
    "            val_loader=DataLoader(val_dataset, batch_size = batch_size, shuffle=False, num_workers=8)\n",
    "            model, val_epoch_loss, batch_val_logs_df = val_logic(model, epoch, val_loader, model_logs_dir)\n",
    "        elif 21 <= cycle_epoch <= 30:\n",
    "            #train_dataset_args_dict=random.choice(train_dataset_args_choices)\n",
    "            train_dataset_args_dict=train_dataset_args_choices[1]\n",
    "            train_dataset=CamelyonDataset(**train_dataset_args_dict, augmentations=train_augs)\n",
    "            train_loader=DataLoader(train_dataset, batch_size=batch_size, shuffle=False, num_workers=8)\n",
    "            model, train_epoch_loss, batch_train_logs_df=train_logic(model, epoch, train_loader, model_logs_dir)\n",
    "    \n",
    "            #val_dataset_args_dict=random.choice(val_dataset_args_choices)\n",
    "            val_dataset_args_dict=val_dataset_args_choices[1]\n",
    "            val_dataset=CamelyonDataset(**val_dataset_args_dict)\n",
    "            val_loader=DataLoader(val_dataset, batch_size = batch_size, shuffle=False, num_workers=8)\n",
    "            model, val_epoch_loss, batch_val_logs_df = val_logic(model, epoch, val_loader, model_logs_dir)\n",
    "        elif 31 <= cycle_epoch <= 40:\n",
    "            #train_dataset_args_dict=random.choice(train_dataset_args_choices)\n",
    "            train_dataset_args_dict=train_dataset_args_choices[0]\n",
    "            train_dataset=CamelyonDataset(**train_dataset_args_dict, augmentations=train_augs)\n",
    "            train_loader=DataLoader(train_dataset, batch_size=batch_size, shuffle=False, num_workers=8)\n",
    "            model, train_epoch_loss, batch_train_logs_df=train_logic(model, epoch, train_loader, model_logs_dir)\n",
    "    \n",
    "            #val_dataset_args_dict=random.choice(val_dataset_args_choices)\n",
    "            val_dataset_args_dict=val_dataset_args_choices[0]\n",
    "            val_dataset=CamelyonDataset(**val_dataset_args_dict)\n",
    "            val_loader=DataLoader(val_dataset, batch_size = batch_size, shuffle=False, num_workers=8)\n",
    "            model, val_epoch_loss, batch_val_logs_df = val_logic(model, epoch, val_loader, model_logs_dir)\n",
    "        elif 41 <= cycle_epoch <= 50 or cycle_epoch == 0:\n",
    "            train_dataset_args_dict=random.choice(train_dataset_args_choices)\n",
    "            #train_dataset_args_dict=train_dataset_args_choices[3]\n",
    "            train_dataset=CamelyonDataset(**train_dataset_args_dict, augmentations=train_augs)\n",
    "            train_loader=DataLoader(train_dataset, batch_size=batch_size, shuffle=False, num_workers=8)\n",
    "            model, train_epoch_loss, batch_train_logs_df=train_logic(model, epoch, train_loader, model_logs_dir)\n",
    "    \n",
    "            val_dataset_args_dict=random.choice(val_dataset_args_choices)\n",
    "            #val_dataset_args_dict=val_dataset_args_choices[3]\n",
    "            val_dataset=CamelyonDataset(**val_dataset_args_dict)\n",
    "            val_loader=DataLoader(val_dataset, batch_size = batch_size, shuffle=False, num_workers=8)\n",
    "            model, val_epoch_loss, batch_val_logs_df = val_logic(model, epoch, val_loader, model_logs_dir)\n",
    "    \n",
    "        train_dice_score=1-batch_train_logs_df['dice_loss'].mean()\n",
    "        val_dice_score=1-batch_val_logs_df['dice_loss'].mean()\n",
    "        scheduler.step(val_dice_score)\n",
    "    \n",
    "        epoch_temp_dict={}    \n",
    "        epoch_temp_dict['epoch']=epoch\n",
    "        epoch_temp_dict['avg_train_loss']=train_epoch_loss/len(train_dataset)\n",
    "        epoch_temp_dict['avg_val_loss']=val_epoch_loss/len(val_dataset)\n",
    "        epoch_temp_dict['train_dice']=train_dice_score\n",
    "        epoch_temp_dict['val_dice']=val_dice_score\n",
    "        epoch_temp_dict['learning rate']=optimizer.param_groups[0]['lr']\n",
    "        \n",
    "        epoch_temp_dict['train_loss']=train_epoch_loss\n",
    "        epoch_temp_dict['val_loss']=val_epoch_loss\n",
    "        epoch_temp_dict['train_mode']=train_dataset_args_dict['training_mode']\n",
    "        epoch_temp_dict['train_slice_tumor']=train_dataset_args_dict['slice_tumor']\n",
    "        epoch_temp_dict['train_sample_without_replacement']=train_dataset_args_dict['sample_without_replacement']\n",
    "        epoch_temp_dict['val_mode']=val_dataset_args_dict['training_mode']\n",
    "        epoch_temp_dict['val_slice_tumor']=val_dataset_args_dict['slice_tumor']\n",
    "        epoch_temp_dict['val_sample_without_replacement']=val_dataset_args_dict['sample_without_replacement']\n",
    "    \n",
    "        epoch_logs.append(epoch_temp_dict)\n",
    "        pd.DataFrame(epoch_logs).to_csv(f\"{model_logs_dir}/model_learning_logs.csv\", index=False)\n",
    "     \n",
    "        if epoch==1:\n",
    "            max_val_score=val_dice_score\n",
    "            print(f\"max val score initialised to {round(max_val_score, 2)} for epoch {epoch}.\")\n",
    "            state_dict = model.state_dict()\n",
    "            checkpoint_path=f\"{model_logs_dir}/model_check_points/recurrent/checkpoint_epoch{epoch}_{val_dice_score}.pth\"\n",
    "            torch.save(state_dict, checkpoint_path)\n",
    "            \n",
    "        elif val_dice_score>max_val_score:\n",
    "            max_val_score=val_dice_score\n",
    "            print(f\"max val score changed to {round(max_val_score, 2)} for epoch {epoch}.\")\n",
    "            state_dict = model.state_dict()\n",
    "            checkpoint_path=f\"{model_logs_dir}/model_check_points/max_val/checkpoint_epoch{epoch}_{val_dice_score}.pth\"\n",
    "            torch.save(state_dict, checkpoint_path)\n",
    "            \n",
    "        elif epoch%10==0:\n",
    "            print(f\"max val {round(max_val_score, 2)} score unchanged for epoch {epoch}, val score {val_dice_score}\")\n",
    "            state_dict = model.state_dict()\n",
    "            checkpoint_path=f\"{model_logs_dir}/model_check_points/recurrent/checkpoint_epoch{epoch}_{val_dice_score}.pth\"\n",
    "            torch.save(state_dict, checkpoint_path)\n",
    "        else:\n",
    "            print(f\"max val score unchanged for epoch {epoch}, val score : {round(max_val_score, 2)}\")\n",
    "    except Exception as e:\n",
    "        print(f\"epoch_error:{e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf2ce65f-8860-4fa4-a1e4-aa8f7a889ee6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
